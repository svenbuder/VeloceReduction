{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c53d78",
   "metadata": {},
   "source": [
    "# VeloceReduction -- Tutorial\n",
    "\n",
    "This tutorial provides an example on how to reduce data of a given night YYMMDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b08276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic packages\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# VeloceReduction modules and function\n",
    "from VeloceReduction import config\n",
    "import VeloceReduction as VR\n",
    "\n",
    "from astropy.table import Table\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "from VeloceReduction.utils import polynomial_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982f57a",
   "metadata": {},
   "source": [
    "## Adjust Date and Directory (possibly via argument parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba063e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description=\"Process some inputs.\")\n",
    "    \n",
    "    # Add arguments\n",
    "    parser.add_argument('-d','--date', type=str, default=\"001122\",\n",
    "                        help='Date in the format DDMMYY (e.g., \"001122\")')\n",
    "    parser.add_argument('-wd','--working_directory', type=str, default=\"./\",\n",
    "                        help='The directory where the script will operate.')\n",
    "    \n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def get_script_input():\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        \n",
    "        # Assume default values if inside Jupyter\n",
    "        jupyter_date = \"001122\"\n",
    "        \n",
    "        # 2Amp example\n",
    "#         jupyter_date = \"240219\"\n",
    "        \n",
    "        # 4Amp example\n",
    "#         jupyter_date = \"231121\"\n",
    "        \n",
    "#         jupyter_date = \"240201\"\n",
    "        \n",
    "        jupyter_working_directory = \"./\"\n",
    "        print(\"Running in a Jupyter notebook. Using predefined values\")\n",
    "        args = argparse.Namespace(date=jupyter_date, working_directory=jupyter_working_directory)\n",
    "    else:\n",
    "        # Use argparse to handle command-line arguments\n",
    "        print(\"Running as a standalone Python script\")\n",
    "        args = parse_arguments()\n",
    "\n",
    "    return args\n",
    "\n",
    "# Use the function to get input\n",
    "args = get_script_input()\n",
    "config.date = args.date\n",
    "config.working_directory = args.working_directory\n",
    "print(f\"Date: {args.date}, Working Directory: {args.working_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4baf7e",
   "metadata": {},
   "source": [
    "## Identfiy Calibration and Science Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Calibration and Science data from the night log\n",
    "calibration_runs, science_runs = VR.utils.identify_calibration_and_science_runs(config.date, config.working_directory+'observations/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ac5ac",
   "metadata": {},
   "source": [
    "## Extract orders and save in initial FITS files with an extension per order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7415b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract order ranges and coefficients\n",
    "order_ranges, order_beginning_coeffs, order_ending_coeffs = VR.extraction.read_in_order_tramlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cccdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Extract Master Flat\n",
    "# print('Extracting Master Flat')\n",
    "# master_flat, noise = VR.extraction.extract_orders(\n",
    "#     ccd1_runs = calibration_runs['Flat_60.0'][:1],\n",
    "#     ccd2_runs = calibration_runs['Flat_1.0'][:1],\n",
    "#     ccd3_runs = calibration_runs['Flat_0.1'][:1],\n",
    "#     Flat = True,\n",
    "#     update_tramlines_based_on_flat = False, # Would update and overwrite\n",
    "#     # ./VeloceReduction/tramline_information/tramline_begin_end_ccd_*_oder_*.txt\n",
    "#     debug_overscan = False,\n",
    "#     debug_tramlines = False # Would create a tramlines trace PDF under\n",
    "#     # ./VeloceReduction/tramline_information/debug_tramlines_flat.pdf\n",
    "# )\n",
    "\n",
    "# # Extract Master ThXe\n",
    "# print('Extracting Master ThXe')\n",
    "# master_thxe, noise = VR.extraction.extract_orders(\n",
    "#     ccd1_runs = calibration_runs['FibTh_180.0'][:1],\n",
    "#     ccd2_runs = calibration_runs['FibTh_60.0'][:1],\n",
    "#     ccd3_runs = calibration_runs['FibTh_15.0'][:1],\n",
    "#     debug_tramlines = False # Would create a tramlines trace PDF under\n",
    "#     # ./VeloceReduction/tramline_information/debug_tramlines.pdf\n",
    "# )\n",
    "\n",
    "# # Extract Master LC\n",
    "# print('Extracting Master LC')\n",
    "# master_lc, noise = VR.extraction.extract_orders(\n",
    "#     ccd1_runs = calibration_runs['SimLC'][-1:],\n",
    "#     ccd2_runs = calibration_runs['SimLC'][-1:],\n",
    "#     ccd3_runs = calibration_runs['SimLC'][-1:],\n",
    "#     LC = True,\n",
    "#     debug_tramlines = False # Would create a tramlines trace PDF under\n",
    "#     # ./VeloceReduction/tramline_information/debug_tramlines_lc.pdf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56105fe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Extract Science Objects and save them into FITS files under reduced_data/\n",
    "# for science_object in list(science_runs.keys()):\n",
    "#     print('Extracting '+science_object)\n",
    "#     try:\n",
    "#         science, science_noise, science_header = VR.extraction.extract_orders(\n",
    "#             ccd1_runs = science_runs[science_object],\n",
    "#             ccd2_runs = science_runs[science_object],\n",
    "#             ccd3_runs = science_runs[science_object],\n",
    "#             Science=True,\n",
    "#             debug_tramlines = False, # Would create a tramlines trace PDF under\n",
    "#             # ./VeloceReduction/tramline_information/debug_tramlines_science.pdf\n",
    "#             debug_overscan=False\n",
    "#         )\n",
    "\n",
    "#         # Create a primary HDU and HDU list\n",
    "#         primary_hdu = fits.PrimaryHDU()\n",
    "#         header = primary_hdu.header\n",
    "#         header['OBJECT']             = (science_header['OBJECT'], 'Name of observed object in night log')\n",
    "#         header['HIERARCH SOURCE_ID'] = (-1,                       'Gaia DR3 source_id')\n",
    "#         header['HIERARCH TMASS_ID']  = ('HHMMSSSS-DDMMSSS',       'Identifier in 2MASS catalog')\n",
    "#         header['UTMJD']              = (science_header['UTMJD'],  'Modified Julian Date of observation')\n",
    "#         header['MEANRA']             = (science_header['MEANRA'], 'Mean Right Ascension of observed object')\n",
    "#         header['MEANDEC']            = (science_header['MEANDEC'],'Mean Declination of observed object')        \n",
    "#         header['BARYVEL']            = ('None',                   'Applied barycentric velocity correction')\n",
    "#         header['VRAD']               = ('None',                   'Radial velocity estimate')\n",
    "#         header['E_VRAD']             = ('None',                   'Uncertainty of radial velocity estimate')\n",
    "#         hdul = fits.HDUList([primary_hdu])\n",
    "\n",
    "#         # Loop over your extension names and corresponding data arrays\n",
    "#         for ext_index, ext_name in enumerate(order_beginning_coeffs):\n",
    "#             # Create an ImageHDU object for each extension\n",
    "            \n",
    "#             # Apply flat-field calibration to science\n",
    "#             science[ext_index,:] /= master_flat[ext_index,:]\n",
    "#             science_noise[ext_index,:] /= master_flat[ext_index,:]\n",
    "            \n",
    "#             # Apply rough renormalisation with outlier-robuster 99th percenile of ~middle of order\n",
    "#             science_99percentile = np.nanpercentile(science[ext_index,1500:2500],q=99)\n",
    "#             science[ext_index,:] /= science_99percentile\n",
    "#             science_noise[ext_index,:] /= science_99percentile\n",
    "            \n",
    "#             # Define the columns with appropriate formats\n",
    "#             col1_def = fits.Column(name='wave_vac',format='E', array=np.arange(len(science[ext_index,:]),dtype=float))\n",
    "#             col2_def = fits.Column(name='wave_air',format='E', array=np.arange(len(science[ext_index,:]),dtype=float))\n",
    "#             col3_def = fits.Column(name='science', format='E', array=science[ext_index,:])\n",
    "#             col4_def = fits.Column(name='science_noise',   format='E', array=science_noise[ext_index,:])\n",
    "#             col5_def = fits.Column(name='flat',    format='E', array=master_flat[ext_index,:])\n",
    "#             col6_def = fits.Column(name='thxe',    format='E', array=master_thxe[ext_index,:]/master_flat[ext_index,:])\n",
    "#             col7_def = fits.Column(name='lc',      format='E', array=master_lc[ext_index,:]/master_flat[ext_index,:])\n",
    "\n",
    "#             # Combine columns to BinTable and add header from primary\n",
    "#             hdu = fits.BinTableHDU.from_columns([col1_def, col2_def, col3_def, col4_def, col5_def, col6_def, col7_def], name=ext_name.lower())\n",
    "#             hdu.header.extend(header.copy(strip=True), unique=True)\n",
    "            \n",
    "#             # Append the HDU to the HDU list\n",
    "#             hdul.append(hdu)\n",
    "\n",
    "#         # Save to a new FITS file with an extension for each order\n",
    "#         Path(config.working_directory+'reduced_data/'+config.date+'/'+science_object).mkdir(parents=True, exist_ok=True)\n",
    "#         spectrum_filename = 'veloce_spectra_'+science_object+'_'+config.date+'.fits'\n",
    "#         hdul.writeto(config.working_directory+'reduced_data/'+config.date+'/'+science_object+'/'+spectrum_filename, overwrite=True)\n",
    "\n",
    "#         print('  -> Successfully extracted '+science_object)\n",
    "\n",
    "#     except:\n",
    "#         print('  -> Failed to extract '+science_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a7445",
   "metadata": {},
   "source": [
    "## Wavelength calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b6d6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for science_object in list(science_runs.keys()):\n",
    "#     try:\n",
    "    VR.calibration.calibrate_wavelength(\n",
    "        science_object,\n",
    "        correct_barycentric_velocity=True,\n",
    "        create_overview_pdf=False\n",
    "    )\n",
    "#         print('  -> Succesfully calibrated wavelength with diagnostic plots for '+science_object)\n",
    "#     except:\n",
    "#         print('  -> Failed to calibrate wavelength for '+science_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf9a37",
   "metadata": {},
   "source": [
    "## DELETE WHEN DONE! LASER COMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2219489",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_object = 'HIP69673'\n",
    "input_output_directory = config.working_directory+'reduced_data/'+config.date+'/'+science_object\n",
    "\n",
    "spectrum = dict()\n",
    "\n",
    "with fits.open(input_output_directory+'/veloce_spectra_'+science_object+'_'+config.date+'.fits', mode='update') as file:\n",
    "    \n",
    "    for index in range(1,len(file)):\n",
    "\n",
    "        spectrum[file[index].header['EXTNAME'].lower()] = Table()\n",
    "        for key in ['wave_vac','wave_air','science','science_noise','flat','thxe','lc']:\n",
    "            spectrum[file[index].header['EXTNAME'].lower()][key] = file[index].data[key]\n",
    "            \n",
    "coeffs_tinney = VR.utils.read_in_wavelength_solution_coefficients_tinney()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179e9b8",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lc_range = dict()\n",
    "lc_range['ccd_3_order_104'] = [1455,3900]\n",
    "lc_range['ccd_3_order_103'] = [780,3900]\n",
    "lc_range['ccd_3_order_102'] = [700,3900]\n",
    "lc_range['ccd_3_order_101'] = [650,3920]\n",
    "lc_range['ccd_3_order_100'] = [600,3925]\n",
    "lc_range['ccd_3_order_99'] = [490,3925]\n",
    "lc_range['ccd_3_order_98'] = [500,3925]\n",
    "lc_range['ccd_3_order_97'] = [325,3825]\n",
    "lc_range['ccd_3_order_96'] = [255,3850]\n",
    "lc_range['ccd_3_order_95'] = [250,3650]\n",
    "lc_range['ccd_3_order_94'] = [450,3800]\n",
    "lc_range['ccd_3_order_93'] = [270,3700]\n",
    "lc_range['ccd_3_order_92'] = [540,2800]\n",
    "lc_range['ccd_3_order_91'] = [630,3525]\n",
    "lc_range['ccd_3_order_90'] = [490,3525]\n",
    "lc_range['ccd_3_order_89'] = [355,3465]\n",
    "lc_range['ccd_3_order_88'] = [400,2900]\n",
    "lc_range['ccd_3_order_87'] = [505,3245]\n",
    "lc_range['ccd_3_order_86'] = [200,3200]\n",
    "lc_range['ccd_3_order_85'] = [455,3655]\n",
    "lc_range['ccd_3_order_84'] = [150,3578]\n",
    "lc_range['ccd_3_order_83'] = [145,3850]\n",
    "lc_range['ccd_3_order_82'] = [120,3870]\n",
    "lc_range['ccd_3_order_81'] = [125,4050]\n",
    "lc_range['ccd_3_order_80'] = [130,4000]\n",
    "lc_range['ccd_3_order_79'] = [120,4070]\n",
    "lc_range['ccd_3_order_78'] = [100,4100]\n",
    "lc_range['ccd_3_order_77'] = [100,4100]\n",
    "lc_range['ccd_3_order_76'] = [105,4095]\n",
    "lc_range['ccd_3_order_75'] = [99,4050]\n",
    "lc_range['ccd_3_order_74'] = [745,4050]\n",
    "lc_range['ccd_3_order_73'] = [99,4100]\n",
    "lc_range['ccd_3_order_72'] = [125,4090]\n",
    "lc_range['ccd_3_order_71'] = [95,4086]\n",
    "lc_range['ccd_3_order_70'] = [110,4077]\n",
    "lc_range['ccd_3_order_69'] = [90,4087]\n",
    "lc_range['ccd_3_order_68'] = [90,4090]\n",
    "lc_range['ccd_3_order_67'] = [100,4090]\n",
    "lc_range['ccd_3_order_66'] = [100,3730]\n",
    "\n",
    "# lc_range = dict()\n",
    "\n",
    "# lc_range['ccd_3_order_104'] = [1455,3900]\n",
    "\n",
    "orders = ['ccd_3_order_92']\n",
    "\n",
    "use_ylim = True\n",
    "overwrite = True\n",
    "debug = False\n",
    "\n",
    "# for order in list(spectrum.keys()):\n",
    "for order in lc_range.keys(): \n",
    "# for order in orders: \n",
    "    \n",
    "    if (order[4] != '1') & (order != 'ccd_3_order_65'):\n",
    "\n",
    "        thxe_file_path = './VeloceReduction/veloce_reference_data/thxe_pixels_and_positions/' + order + '_px_wl.txt'\n",
    "        thxe_pixels_and_wavelengths = np.array(np.loadtxt(thxe_file_path))\n",
    "\n",
    "        # Fit a polynomial function to pixel and wavelength data\n",
    "        coeffs_thxe, _ = curve_fit(\n",
    "            polynomial_function,\n",
    "            thxe_pixels_and_wavelengths[:,0] - 2064,\n",
    "            thxe_pixels_and_wavelengths[:,1],\n",
    "            p0=[np.median(thxe_pixels_and_wavelengths[:,1]), 0.05, 0.0, 0.0, 0.0, 0.0]\n",
    "        )\n",
    "        \n",
    "        if order in lc_range.keys():\n",
    "            lc_beginning, lc_ending = lc_range[order]\n",
    "        else:\n",
    "            lc_beginning = 100\n",
    "            lc_ending = 4000\n",
    "            \n",
    "        print(lc_beginning,lc_ending)\n",
    "\n",
    "        # Plot LC positions  \n",
    "        wavelength = spectrum[order]['wave_vac']\n",
    "        wavelength = polynomial_function(np.arange(4128)-2064,*coeffs_thxe)*10\n",
    "        \n",
    "        lc_values = spectrum[order]['lc']\n",
    "        \n",
    "        in_panel = np.arange(lc_beginning,lc_ending+1)\n",
    "\n",
    "        f, ax = plt.subplots(1,1,figsize=(15,5))\n",
    "        ax.set_title(order)\n",
    "        ax.plot(\n",
    "            wavelength,\n",
    "            lc_values,\n",
    "            lw = 0.5\n",
    "        )\n",
    "        ax.set_ylim(0,1.1*np.percentile(lc_values[np.isfinite(lc_values)],q=99))\n",
    "        \n",
    "        peak_distance = 7\n",
    "        peak_height = 20\n",
    "        peak_prominence = 20\n",
    "        \n",
    "        if order in ['ccd_3_order_80','ccd_3_order_81','ccd_3_order_90','ccd_3_order_92']:\n",
    "            peak_height = 10\n",
    "            peak_prominence = 10\n",
    "\n",
    "        if order in ['ccd_3_order_86']:\n",
    "            peak_height = 5\n",
    "            peak_prominence = 5\n",
    "            \n",
    "        if order in ['ccd_3_order_87','ccd_3_order_91']:\n",
    "            peak_height = 1\n",
    "            peak_prominence = 3\n",
    "\n",
    "        if order in ['ccd_3_order_88']:\n",
    "            peak_height = 2\n",
    "            peak_prominence = 2\n",
    "\n",
    "        peaks, peak_metadata = find_peaks(\n",
    "            lc_values[in_panel],\n",
    "            height = peak_height,\n",
    "            prominence = peak_prominence,\n",
    "            distance = peak_distance\n",
    "        )\n",
    "\n",
    "        for peak in peaks:\n",
    "            ax.axvline(wavelength[in_panel][peak], c = 'C3', lw=0.5, ls='dashed')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        fine_peaks = []\n",
    "        \n",
    "        for peak in peaks:\n",
    "            \n",
    "            # Find the pixels that are +- 0.5*peak_distance away from the peak\n",
    "            pixels_around_peak = np.arange(\n",
    "                np.max([0,peak - int(np.ceil(peak_distance/2))]),\n",
    "                np.min([len(lc_values[in_panel]),peak + int(np.ceil(peak_distance/2))+1])\n",
    "            )\n",
    "            pixel_values_around_peak = lc_values[in_panel][pixels_around_peak]\n",
    "            pixel_minmax = list(np.nanpercentile(pixel_values_around_peak,q=[1,99]))\n",
    "            \n",
    "            try:\n",
    "                popt, pcov = curve_fit(\n",
    "                    VR.utils.lc_peak_gauss,\n",
    "                    pixels_around_peak,\n",
    "                    pixel_values_around_peak,\n",
    "                    p0 = [peak, 1, pixel_minmax[1]-pixel_minmax[0], pixel_minmax[0]]\n",
    "                )\n",
    "                \n",
    "                # Make sure the Gaussian is not too far off!\n",
    "                # Use initial peak integer otherwise\n",
    "                if abs(peak - popt[0] > 1):\n",
    "                    fine_peaks.append(peak)\n",
    "                else:\n",
    "                    fine_peaks.append(popt[0])\n",
    "            except:\n",
    "                print('Failed fit for peak '+str(peak)+' at position '+str(peak+lc_beginning))\n",
    "                fine_peaks.append(peak)\n",
    "\n",
    "            if debug:\n",
    "                f, ax = plt.subplots(1,1)\n",
    "                ax.scatter(\n",
    "                    pixels_around_peak,\n",
    "                    pixel_values_around_peak,\n",
    "                    s = 20\n",
    "                )\n",
    "\n",
    "                ax.plot(\n",
    "                    np.linspace(pixels_around_peak[0],pixels_around_peak[-1],50),\n",
    "                    VR.utils.lc_peak_gauss(np.linspace(pixels_around_peak[0],pixels_around_peak[-1],50), *popt),\n",
    "                    c = 'C1'\n",
    "                )\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "        fine_peaks = np.array(fine_peaks)\n",
    "\n",
    "        lc_number_upper = np.floor(VR.utils.lasercomb_numbers_from_wavelength(wavelength[in_panel][0]))\n",
    "        lc_number_lower = np.ceil(VR.utils.lasercomb_numbers_from_wavelength(wavelength[in_panel][-1]))\n",
    "        lc_wavelengths = VR.utils.lasercomb_wavelength_from_numbers(np.arange(lc_number_lower, lc_number_upper+1))[::-1]\n",
    "\n",
    "        print('peaks: ',len(peaks))\n",
    "        print('modes: ',len(lc_wavelengths))\n",
    "        \n",
    "        if len(peaks) != len(lc_wavelengths):\n",
    "            use_peaks_and_modes = np.min([len(peaks),len(lc_wavelengths)])\n",
    "            print('Only using first '+str(use_peaks_and_modes)+' entries')\n",
    "        else:\n",
    "            use_peaks_and_modes = len(peaks)\n",
    "\n",
    "        # for lc_wavelength in lc_wavelengths:\n",
    "        #     ax.axvline(lc_wavelength, c = 'C1', lw = 0.5, ls='dashed')\n",
    "\n",
    "        # Fit a polynomial function to pixel and wavelength data\n",
    "        coeffs_lc, _ = curve_fit(\n",
    "            polynomial_function,\n",
    "            lc_beginning + fine_peaks[:use_peaks_and_modes] - 2064,\n",
    "            lc_wavelengths[:use_peaks_and_modes]/10.,\n",
    "            p0=[np.median(lc_wavelengths), 0.05, 0.0, 0.0, 0.0, 0.0]\n",
    "        )\n",
    "\n",
    "#         print('0th and 4128th pixel:')\n",
    "#         print(polynomial_function(np.array([0,4128])-2064,*coeffs_lc))\n",
    "        \n",
    "#         print(coeffs_lc)\n",
    "        \n",
    "        if overwrite:\n",
    "            np.savetxt('./VeloceReduction/wavelength_coefficients/wavelength_coefficients_'+order+'_lc.txt',coeffs_lc)\n",
    "\n",
    "        rms_wavelength = np.std(lc_wavelengths[:use_peaks_and_modes] - (polynomial_function(lc_beginning+fine_peaks[:use_peaks_and_modes]-2064,*coeffs_lc)*10))\n",
    "        rms_velocity = 299792.46 * np.std((lc_wavelengths[:use_peaks_and_modes] - (polynomial_function(lc_beginning+fine_peaks[:use_peaks_and_modes]-2064,*coeffs_lc)*10))/lc_wavelengths[:use_peaks_and_modes])\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.title(order,fontsize=15)\n",
    "        \n",
    "        plt.scatter(\n",
    "            lc_beginning+fine_peaks[:use_peaks_and_modes],\n",
    "            lc_wavelengths[:use_peaks_and_modes] - (polynomial_function(lc_beginning+fine_peaks[:use_peaks_and_modes]-2064,*coeffs_lc)*10),\n",
    "            s = 1,\n",
    "            label = 'LC Peaks, RMS = '+str(np.round(rms_wavelength,3))+' Å or '+str(np.round(rms_velocity,2))+' km/s'\n",
    "        )\n",
    "        \n",
    "        plt.plot(\n",
    "            np.arange(4128),\n",
    "            polynomial_function(np.arange(4128)-2450-3,*coeffs_tinney[order][:-1])*10 -\n",
    "            polynomial_function(np.arange(4128)-2064,*coeffs_lc)*10,\n",
    "            label = 'Tinney Wavelength Solution'\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(4128),\n",
    "            polynomial_function(np.arange(4128)-2064,*coeffs_thxe)*10 - \n",
    "            polynomial_function(np.arange(4128)-2064,*coeffs_lc)*10,\n",
    "            label = 'ThXe Wavelength Solution'\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(4128),\n",
    "            polynomial_function(np.arange(4128)-2064,*coeffs_lc)*10 - \n",
    "            polynomial_function(np.arange(4128)-2064,*coeffs_lc)*10,\n",
    "            label = 'LC Wavelength Solution'\n",
    "        )\n",
    "        if use_ylim:\n",
    "            plt.ylim(-0.5,0.5)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0088c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6d7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
